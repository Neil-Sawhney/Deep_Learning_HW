data:
  num_words_to_tokenize: 50
display:
  refresh_rate: 5
learning:
  batch_size: 100
  dropout_prob: 0.2
  embedding_depth: 2000
  learning_patience: 0
  learning_rates:
  - 0.0005
  - 0.0001
  - 0.0005
  - 1.0e-05
  num_embeddings: 2000
  num_iters: 100000
  val_check_rate: 10
  weight_decay: 0.2
mlp:
  hidden_layer_width: 300
  num_hidden_layers: 1
