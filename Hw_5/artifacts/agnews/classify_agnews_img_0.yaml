data:
  num_words_to_tokenize: 50
display:
  refresh_rate: 5
learning:
  batch_size: 500
  dropout_prob: 0.2
  embedding_depth: 10
  learning_patience: 20
  learning_rates:
  - 0.0005
  - 0.0001
  - 0.0005
  - 0.001
  - 0.0005
  - 0.0001
  - 5.0e-05
  - 1.0e-05
  num_embeddings: 10
  num_iters: 100000
  val_check_rate: 10
  weight_decay: 0.2
mlp:
  hidden_layer_width: 1000
  num_hidden_layers: 4
